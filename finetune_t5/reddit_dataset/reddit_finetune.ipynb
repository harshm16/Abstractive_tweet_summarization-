{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axiI4RQRoCY4"
      },
      "outputs": [],
      "source": [
        "#Install and import the required packages.\n",
        "\n",
        "# ! pip install datasets transformers rouge-score nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "import tensorflow_datasets as tfds\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "#download directly from the tensorflow datasets repo\n",
        "dataset = load_dataset('reddit',split='train[0%:10%]')\n",
        "\n",
        "#use pre-downloaded dataset.\n",
        "# dataset = load_dataset('json', data_files='C:\\\\Users\\\\mishr\\\\tensorflow_datasets\\\\downloads\\\\extracted\\\\ZIP.zeno.org_reco_1043_file_corp-webwaD4xDdMcxTTyexQ3VBTA8U2Bi2HA31NynA1uJs2k4o.zipdownload=1\\\\corpus-webis-tldr-17.json',split='train[0%:10%]')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#to see how our dataset looks\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Filter posts with summary length greater than 10, but less than 280\n",
        "\n",
        "dataset_needed = dataset.filter(lambda example: example['summary_len'] >= 10 and example['summary_len'] <= 280)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Limit size of posts to 560 words\n",
        "\n",
        "dataset_needed = dataset_needed.filter(lambda example: example['content_len'] >= 80 and example['content_len'] <= 560)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Since we only require the Content and Summary columns\n",
        "\n",
        "updated_dataset = dataset_needed.remove_columns(['author', 'body', 'normalizedBody', 'content_len', 'summary_len', 'id', 'subreddit', 'subreddit_id', 'title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oImhhdFoCY6"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"t5-small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm",
        "outputId": "9f8ead3b-f9fd-4571-88f7-df9264f3c189"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "\n",
        "metric = load_metric(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKTn2OwFoCY_"
      },
      "outputs": [],
      "source": [
        "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
        "    prefix = \"summarize: \"\n",
        "else:\n",
        "    prefix = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "max_input_length = 1024\n",
        "max_target_length = 280\n",
        "\n",
        "#Tokenizes the given text input\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"content\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True,padding=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True,padding=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtsaJeVIrJT",
        "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = updated_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Split data into train and test/eval\n",
        "split_tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItY1F1GsoCZA"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-reddit_small\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size, \n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    # push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsAClfXjoCZB"
      },
      "outputs": [],
      "source": [
        "#takes care of batch formation\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "#Computes the Rouge scores\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    \n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Used to only provide the relevant columns as input to the model\n",
        "\n",
        "columns_to_return = ['input_ids', 'labels', 'attention_mask']\n",
        "split_tokenized_datasets.set_format(type='torch', columns=columns_to_return)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=split_tokenized_datasets['train'],\n",
        "    eval_dataset=split_tokenized_datasets['test'],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx5pyRlIrJh",
        "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#Start training the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#To decode predictions on the test data set\n",
        "predictions = trainer.predict(split_tokenized_datasets[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "#Decodes the tokenized text\n",
        "def decode_labels(predictions, labels):\n",
        "    predictions, labels = predictions, labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "\n",
        "    \n",
        "    return decoded_preds, decoded_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#d_preds has the decoded predicted summary, d_labels has the decoded golden summary.\n",
        "\n",
        "d_preds, d_labels = decode_labels(predictions.predictions, predictions.label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#To test the model on custom input. Use input_tweet to provide text to be summarized.\n",
        "\n",
        "input_tweet = \"Dejan Kulusevski has created at least 1 shooting opportunity from inside the box for his teammates in 6 consecutive Premier League games now. Not a single shot for Dejan Kulusevski today, but his streak of creating at least 1 shooting chance from inside the penalty area for his teammates increases to 5 PL games now. Dejan Kulusevski received the most number of passes amongst Spurs' front 3. Even if you ignore the high quality chance he created for Sonny, if the player playing alongside Kane and Son, passes them the ball 30 of the times and only has 3/45 unsuccessful passes, his job is done.\"\n",
        "\n",
        "\n",
        "inputs = tokenizer(input_tweet, max_length=5024, return_tensors=\"pt\")\n",
        "\n",
        "# Generate Summary\n",
        "summary_ids = model.generate(inputs[\"input_ids\"].cuda())\n",
        "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#To compare the summaries generated by the 2 models. Use tweets from csv files to extract keywords.\n",
        "\n",
        "all_files = ['neutral_arsenal.csv', 'negative_aston_vila.csv', 'negative_chelsea.csv', 'neutral_aston_vila.csv', 'neutral_chelsea.csv',\n",
        "'positive_arsenal.csv', 'positive_aston_vila.csv', 'positive_chelsea.csv','neutral_everton.csv', 'neutral_leeds.csv', 'neutral_leicester.csv', 'positive_everton.csv', \n",
        "'positive_leeds.csv', 'positive_leicester.csv','negative_leicester.csv', 'negative_leeds.csv', 'negative_everton.csv']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Finds if the keywords for the respective tweets are present in the summarized tweet or not\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "all_values = []\n",
        "for files in all_files:\n",
        "    path = '..\\\\..\\\\data\\\\' + files\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # print(df) \n",
        "\n",
        "    summary = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        inputs = tokenizer(row[1], max_length=1024, return_tensors=\"pt\")\n",
        "\n",
        "        # Generate Summary\n",
        "        # summary_ids = model.generate(inputs[\"input_ids\"].cuda())\n",
        "\n",
        "        summary_ids = model.generate(inputs[\"input_ids\"],min_length = 10,max_length=50)\n",
        "        summary.append(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n",
        "\n",
        "    df['summary'] = summary\n",
        "\n",
        "    count = 0\n",
        "    for index, row in df.iterrows():\n",
        "        res = re.sub(r'[^a-zA-Z]', ' ', row[2])\n",
        "        for each in res.split(\"  \"):\n",
        "            if each:\n",
        "                if each in row['summary']:\n",
        "                    count = count +1\n",
        "                    break\n",
        "\n",
        "    print(\"len\", len(df))\n",
        "    print(\"count\",count)\n",
        "\n",
        "    all_values.append(count/len(df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Summarization",
      "provenance": []
    },
    "interpreter": {
      "hash": "d61e1109225bf29d6c16a9bc24f38ed9db671ded9b9d09d5c99f491a5ccf2da8"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('new')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
